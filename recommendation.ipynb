{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy\n",
    "from stemming.porter2 import stem\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "all_stopwords = sp.Defaults.stop_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('tfidf_vectorizer.pkl', 'rb') as f:\n",
    "    vectorizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    #print('processing')\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+',' ', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9 ]', '', text).strip()\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text).strip()\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    \n",
    "    pure_words_list = remove_stops(text)\n",
    "    stemmed_text = stem_all_words(pure_words_list)\n",
    "    \n",
    "    return stemmed_text\n",
    "\n",
    "\n",
    "def remove_stops(text):\n",
    "    \n",
    "    text_tokens = word_tokenize(text)\n",
    "    tokens_without_sw= [word for word in text_tokens if not word in all_stopwords]\n",
    "\n",
    "    return tokens_without_sw\n",
    "\n",
    "def stem_all_words(text):\n",
    "    \n",
    "    w = [stem(word) for word in text]\n",
    "    return ' '.join(w)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_about_me = '''\n",
    "I am a fourth year engineering student at VIT University pursuing my computer science degree. I am currently into the fields of machine learning,\n",
    " natural language processing and artificial intelligence. I have 4 years of experience with python and\n",
    " I am looking forward to work on such projects in the future\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_1 = '''\n",
    "I am a front-end developer with 5 years of experience working with e-commerce companies.I specialize in using Java, PHP,\n",
    "and Ruby to build customer facing APIs, and also have experience integrating payment systems.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_2 = '''\n",
    "Iâ€™m a web developer from Mumbai, India. I focus on front-end web development to bring the best experience to your users. I have worked on many \n",
    "frontend and backend frameworks like Django, NodeJs etc.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_3 = '''\n",
    "Highly organized and detail-oriented honors graduate from the University of Georgia seeking an entry-level position\n",
    "as an accountant. Served as a peer tutor for courses such as general accounting, budgeting and forecasting, \n",
    "and accounting principles and legislation.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_4 = '''\n",
    "I am a student pursuing to be an engineer at Manipal Institue of technology. I like working in Machine learning and Artificial Intelligence projects.\n",
    "I am still a beginner but I want to be a successfull AI enthusiast in the future'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_5 = '''\n",
    "I'm a senior software engineer working in Novartis working on machine learning and natural language processing. I am looking to work with students\n",
    "pursuing engineering for exciting projects\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = {'1': person_1, '2': person_2, '3': person_3, '4': person_4, '5': person_5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(person: str, rest_vectors: dict):\n",
    "    \"\"\"\n",
    "        Return first 5 vectors above 70% score\n",
    "    \"\"\"\n",
    "    \n",
    "    ids = []\n",
    "    descriptions = []\n",
    "    for key, value in rest_vectors.items():\n",
    "        \n",
    "        ids.append(key)\n",
    "        descriptions.append(value)\n",
    "        \n",
    "    descriptions.insert(0,person)\n",
    "    \n",
    "    d = [preprocess_text(i) for i in descriptions]\n",
    "    descriptions = d\n",
    "    \n",
    "    tfidf_term_vectors  = vectorizer.transform(descriptions)\n",
    "    \n",
    "   \n",
    "    tfidf_term_vectors = tfidf_term_vectors.toarray()\n",
    "    \n",
    "    similarities = []\n",
    "    for i in range(1,len(rest_vectors)+1):\n",
    "        similarities.append(cosine_similarity(np.array([tfidf_term_vectors[0]]), np.array([tfidf_term_vectors[i]])))\n",
    "        \n",
    "    ans = [x for _,x in sorted(zip(similarities,ids))]\n",
    "    \n",
    "    \n",
    "    return ans[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-a18bc90ece37>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msutiable_person_id_recommendation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecommend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_about_me\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-d7d2b962767c>\u001b[0m in \u001b[0;36mrecommend\u001b[1;34m(person, rest_vectors)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mdescriptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mperson\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdescriptions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mdescriptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-d7d2b962767c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mdescriptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mperson\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdescriptions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mdescriptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-00fbda20bfa9>\u001b[0m in \u001b[0;36mpreprocess_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' +'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mpure_words_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_stops\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mstemmed_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstem_all_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpure_words_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-00fbda20bfa9>\u001b[0m in \u001b[0;36mremove_stops\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mremove_stops\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mtext_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mtokens_without_sw\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext_tokens\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_stopwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "sutiable_person_id_recommendation = recommend(my_about_me,database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sutiable_person_id_recommendation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' User descroption of himself is : \\n {my_about_me}')\n",
    "print( '_'*100)\n",
    "\n",
    "print('Recommendations from best to worst: \\n')\n",
    "for i in range(len(sutiable_person_id_recommendation)):\n",
    "    print(f' Person {sutiable_person_id_recommendation[i]} : \\n Description: {database[sutiable_person_id_recommendation[i]]} \\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
